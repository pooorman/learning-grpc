// Protobuf definition of generic model inference server.

syntax = "proto3";

import "tensorflow/core/framework/tensor.proto";

package tensorflow.serving;

message InferenceRequest {
  string model = 1;
  int32 version = 2;
  map<string, TensorProto> inputs = 3;
}

message InferenceResponse {
  map<string, TensorProto> outputs = 1;
};

service InferenceService {
  rpc Inference(InferenceRequest) returns (InferenceResponse);
}
